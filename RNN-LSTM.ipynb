{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "  RNN单步前向传播，使用tanh激活单元\n",
    "  Inputs:\n",
    "  - x: 当前时间步数据输入(N, D).\n",
    "  - prev_h: 前一时间步隐藏层状态 (N, H)\n",
    "  - Wx: 输入层到隐藏层连接权重(D, H)\n",
    "  - Wh:隐藏层到隐藏层连接权重(H, H)\n",
    "  - b: 隐藏层偏置项(H,)\n",
    "\n",
    "  Returns 元组:\n",
    "  - next_h: 下一隐藏层状态(N, H)\n",
    "  - cache: 缓存\n",
    "  \"\"\"\n",
    "    next_h, cache = None, None\n",
    "    #######################################\n",
    "    #                 任务：实现RNN单步前向传播                                  \n",
    "    #               将输出值储存在next_h中，                                     \n",
    "    #         将反向传播时所需的各项缓存存放在cache中                            \n",
    "    #######################################\n",
    "    a = prev_h.dot(Wh) + x.dot(Wx) + b\n",
    "    next_h = np.tanh(a)\n",
    "    cache = (x, prev_h, Wh, Wx, b, next_h)\n",
    "    #######################################\n",
    "    #                             结束编码                                       \n",
    "    #######################################\n",
    "    return next_h, cache\n",
    "\n",
    "def rnn_step_backward(dnext_h, cache):\n",
    "    \"\"\"\n",
    "  RNN单步反向传播。\n",
    "  Inputs:\n",
    "  - dnext_h: 后一时间片段的梯度。\n",
    "  - cache: 前向传播时的缓存。\n",
    "  \n",
    "  Returns 元组:\n",
    "  - dx: 数据梯度(N, D)。\n",
    "  - dprev_h: 前一时间片段梯度(N, H)。\n",
    "  - dWx: 输入层到隐藏层权重梯度(D,H)。\n",
    "  - dWh:  隐藏层到隐藏层权重梯度(H, H)。\n",
    "  - db: 偏置项梯度(H,)。\n",
    "  \"\"\"\n",
    "    dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n",
    "    #######################################\n",
    "    #              任务：实现RNN单步反向传播                                     \n",
    "    #      提示：tanh(x)梯度:  1 - tanh(x)*tanh(x)                               \n",
    "    #######################################\n",
    "\n",
    "    x, prev_h, Wh, Wx, b, next_h = cache\n",
    "    dscores = dnext_h * (1 - next_h * next_h)  # tanh\n",
    "    dWx = np.dot(x.T, dscores)\n",
    "    db = np.sum(dscores, axis=0)\n",
    "    dWh = np.dot(prev_h.T, dscores)\n",
    "    dx = np.dot(dscores, Wx.T)\n",
    "    dprev_h = np.dot(dscores, Wh.T)\n",
    "\n",
    "    #######################################\n",
    "    #                               结束编码                                     \n",
    "    #######################################\n",
    "    return dx, dprev_h, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "  RNN前向传播。\n",
    "  Inputs:\n",
    "  - x: 完整的时序数据 (N, T, D)。\n",
    "  - h0: 隐藏层初始化状态 (N, H)。\n",
    "  - Wx: 输入层到隐藏层权重 (D, H)。\n",
    "  - Wh:  隐藏层到隐藏层权重(H, H)。\n",
    "  - b: 偏置项(H,)。\n",
    "  \n",
    "  Returns 元组:\n",
    "  - h: 所有时间步隐藏层状态(N, T, H)。\n",
    "  - cache: 反向传播所需的缓存。\n",
    "  \"\"\"\n",
    "    h, cache = None, None\n",
    "    #######################################\n",
    "    #                     任务：实现RNN前向传播。                                \n",
    "    #        提示： 使用前面实现的rnn_step_forward 函数。                        \n",
    "    #######################################\n",
    "    N, T, D = x.shape\n",
    "    (H, ) = b.shape\n",
    "    h = np.zeros((N, T, H))\n",
    "    prev_h = h0\n",
    "    for t in range(T):\n",
    "        xt = x[:, t, :]\n",
    "        next_h, _ = rnn_step_forward(xt, prev_h, Wx, Wh, b)\n",
    "        prev_h = next_h\n",
    "        h[:, t, :] = prev_h\n",
    "    cache = (x, h0, Wh, Wx, b, h)\n",
    "    #######################################\n",
    "    #                           结束编码                                         #\n",
    "    #######################################\n",
    "    return h, cache\n",
    "\n",
    "\n",
    "def rnn_backward(dh, cache):\n",
    "    \"\"\"\n",
    "  RNN反向传播。\n",
    "  Inputs:\n",
    "  - dh: 隐藏层所有时间步梯度(N, T, H)。\n",
    "  Returns 元组:\n",
    "  - dx: 输入数据时序梯度(N, T, D)。\n",
    "  - dh0: 初始隐藏层梯度(N, H)。\n",
    "  - dWx: 输入层到隐藏层权重梯度(D, H)。\n",
    "  - dWh: 隐藏层到隐藏层权重梯度(H, H)。\n",
    "  - db: 偏置项梯度(H,)。\n",
    "  \"\"\"\n",
    "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "    #######################################\n",
    "    #                任务：实现RNN反向传播。                                     \n",
    "    #            提示：使用 rnn_step_backward函数。                              \n",
    "    #######################################\n",
    "    x, h0, Wh, Wx, b, h = cache\n",
    "    N, T, H = dh.shape\n",
    "    _, _, D = x.shape\n",
    "    \n",
    "    next_h = h[:, T - 1, :]  # 最后一步\n",
    "    dprev_h = np.zeros((N, H))\n",
    "    \n",
    "    dx = np.zeros((N, T, D))\n",
    "    dh0 = np.zeros((N, H))\n",
    "    dWx = np.zeros((D, H))\n",
    "    dWh = np.zeros((H, H))\n",
    "    db = np.zeros((H, ))\n",
    "    \n",
    "    for t in range(T):\n",
    "        t = T - 1 - t\n",
    "        xt = x[:, t, :]\n",
    "        \n",
    "        if t == 0:\n",
    "            prev_h = h0\n",
    "        else:\n",
    "            prev_h = h[:, t - 1, :]  # 前一层\n",
    "        \n",
    "        step_cache = (xt, prev_h, Wh, Wx, b, next_h)\n",
    "        next_h = prev_h  # 下次循环的前一层\n",
    "        \n",
    "        dnext_h = dh[:, t, :] + dprev_h  # 前一层梯度，当前层输入梯度之和\n",
    "        dx[:, t, :], dprev_h, dWxt, dWht, dbt = rnn_step_backward(\n",
    "            dnext_h, step_cache)\n",
    "        dWx, dWh, db = dWx + dWxt, dWh + dWht, db + dbt\n",
    "    \n",
    "    dh0 = dprev_h\n",
    "    #######################################\n",
    "    #                               结束编码                                     #\n",
    "    #######################################\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "  数值稳定版本的sigmoid函数。\n",
    "  \"\"\"\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "    z = np.zeros_like(x)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])\n",
    "    top = np.ones_like(x)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    return top / (1 + z)\n",
    "\n",
    "\n",
    "def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "  LSTM单步前向传播\n",
    "  \n",
    "  Inputs:\n",
    "  - x: 输入数据 (N, D)\n",
    "  - prev_h: 前一隐藏层状态 (N, H)\n",
    "  - prev_c: 前一细胞状态(N, H)\n",
    "  - Wx: 输入层到隐藏层权重(D, 4H)\n",
    "  - Wh: 隐藏层到隐藏层权重 (H, 4H)\n",
    "  - b: 偏置项(4H,)\n",
    "  \n",
    "  Returns 元组:\n",
    "  - next_h:  下一隐藏层状态(N, H)\n",
    "  - next_c:  下一细胞状态(N, H)\n",
    "  - cache: 反向传播所需的缓存\n",
    "  \"\"\"\n",
    "    next_h, next_c, cache = None, None, None\n",
    "    #######################################\n",
    "    #              任务：实现LSTM单步前向传播。                                 \n",
    "    #         提示：稳定版本的sigmoid函数已经帮你实现，直接调用即可。           \n",
    "    #               tanh函数使用np.tanh。                                       \n",
    "    #######################################\n",
    "    N, D = x.shape\n",
    "    N, H = prev_h.shape\n",
    "    \n",
    "    input_gate = sigmoid(\n",
    "        np.dot(x, Wx[:, 0:H]) + np.dot(prev_h, Wh[:, 0:H]) + b[0:H])\n",
    "    forget_gate = sigmoid(\n",
    "        np.dot(x, Wx[:, H:2 * H]) + np.dot(prev_h, Wh[:, H:2 * H]) +\n",
    "        b[H:2 * H])\n",
    "    output_gate = sigmoid(\n",
    "        np.dot(x, Wx[:, 2 * H:3 * H]) + np.dot(prev_h, Wh[:, 2 * H:3 * H]) +\n",
    "        b[2 * H:3 * H])\n",
    "    \n",
    "    inputs = np.tanh(\n",
    "        np.dot(x, Wx[:, 3 * H:4 * H]) + np.dot(prev_h, Wh[:, 3 * H:4 * H]) +\n",
    "        b[3 * H:4 * H])\n",
    "    \n",
    "    next_c = forget_gate * prev_c + inputs * input_gate\n",
    "    next_scores_c = np.tanh(next_c)\n",
    "    next_h = output_gate * next_scores_c\n",
    "    cache = (x, Wx, Wh, b, inputs, input_gate, output_gate, forget_gate, prev_h,\n",
    "             prev_c, next_scores_c)\n",
    "    #######################################\n",
    "    #                               结束编码                                     #\n",
    "    #######################################\n",
    "\n",
    "    return next_h, next_c, cache\n",
    "\n",
    "\n",
    "def lstm_step_backward(dnext_h, dnext_c, cache):\n",
    "    \"\"\"\n",
    "   LSTM单步反向传播\n",
    "  \n",
    "  Inputs:\n",
    "  - dnext_h: 下一隐藏层梯度 (N, H)\n",
    "  - dnext_c: 下一细胞梯度 (N, H)\n",
    "  - cache: 前向传播缓存\n",
    "  \n",
    "  Returns 元组:\n",
    "  - dx: 输入数据梯度 (N, D)\n",
    "  - dprev_h: 前一隐藏层梯度 (N, H)\n",
    "  - dprev_c: 前一细胞梯度(N, H)\n",
    "  - dWx: 输入层到隐藏层梯度(D, 4H)\n",
    "  - dWh:  隐藏层到隐藏层梯度(H, 4H)\n",
    "  - db:  偏置梯度(4H,)\n",
    "  \"\"\"\n",
    "    dx, dprev_h, dc, dWx, dWh, db = None, None, None, None, None, None\n",
    "    #######################################\n",
    "    #                           任务：实现LSTM单步反向传播                      \n",
    "    #          提示：sigmoid(x)函数梯度：sigmoid(x)*(1-sigmoid(x))              \n",
    "    #                tanh(x)函数梯度：   1-tanh(x)*tanh(x)                      \n",
    "    #######################################\n",
    "    x, Wx, Wh, b, inputs, input_gate, output_gate, forget_gate, prev_h, prev_c, next_scores_c = cache\n",
    "    N, D = x.shape\n",
    "    N, H = prev_h.shape\n",
    "    \n",
    "    dWx = np.zeros((D, 4 * H))\n",
    "    dxx = np.zeros((D, 4 * H))\n",
    "    dWh = np.zeros((H, 4 * H))\n",
    "    dhh = np.zeros((H, 4 * H))\n",
    "    db = np.zeros(4 * H)\n",
    "    dx = np.zeros((N, D))\n",
    "    dprev_h = np.zeros((N, H))\n",
    "    \n",
    "    dc_tem = dnext_c + dnext_h * (1 - next_scores_c**2) * output_gate\n",
    "    dprev_c = forget_gate * dc_tem\n",
    "    \n",
    "    dforget_gate = prev_c * dc_tem\n",
    "    dinput_gate = inputs * dc_tem\n",
    "    dinput = input_gate * dc_tem\n",
    "    doutput_gate = next_scores_c * dnext_h\n",
    "    \n",
    "    dscores_in_gate = input_gate * (1 - input_gate) * dinput_gate\n",
    "    dscores_forget_gate = forget_gate * (1 - forget_gate) * dforget_gate\n",
    "    dscores_out_gate = output_gate * (1 - output_gate) * doutput_gate\n",
    "    dscores_in = (1 - inputs**2) * dinput\n",
    "    \n",
    "    da = np.hstack(\n",
    "        (dscores_in_gate, dscores_forget_gate, dscores_out_gate, dscores_in))\n",
    "    \n",
    "    dWx = np.dot(x.T, da)\n",
    "    dWh = np.dot(prev_h.T, da)\n",
    "    \n",
    "    db = np.sum(da, axis=0)\n",
    "    dx = np.dot(da, Wx.T)\n",
    "    dprev_h = np.dot(da, Wh.T)\n",
    "    #######################################\n",
    "    #                               结束编码                                     #\n",
    "    #######################################\n",
    "\n",
    "    return dx, dprev_h, dprev_c, dWx, dWh, db\n",
    "\n",
    "\n",
    "def lstm_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "  LSTM前向传播\n",
    "  Inputs:\n",
    "  - x: 输入数据 (N, T, D)\n",
    "  - h0:初始化隐藏层状态(N, H)\n",
    "  - Wx: 输入层到隐藏层权重 (D, 4H)\n",
    "  - Wh: 隐藏层到隐藏层权重(H, 4H)\n",
    "  - b: 偏置项(4H,)\n",
    "  \n",
    "  Returns 元组:\n",
    "  - h: 隐藏层所有状态 (N, T, H)\n",
    "  - cache: 用于反向传播的缓存\n",
    "  \"\"\"\n",
    "    h, cache = None, None\n",
    "    #######################################\n",
    "    #                    任务： 实现完整的LSTM前向传播                          \n",
    "    #######################################\n",
    "    N, T, D = x.shape\n",
    "    H = b.shape[0] / 4\n",
    "    h = np.zeros((N, T, H))\n",
    "    cache = {}\n",
    "    prev_h = h0\n",
    "    prev_c = np.zeros((N, H))\n",
    "    for t in range(T):\n",
    "        xt = x[:, t, :]\n",
    "        next_h, next_c, cache[t] = lstm_step_forward(xt, prev_h, prev_c, Wx,\n",
    "                                                     Wh, b)\n",
    "        prev_h = next_h\n",
    "        prev_c = next_c\n",
    "        h[:, t, :] = prev_h\n",
    "\n",
    "    #######################################\n",
    "    #                               结束编码                                     #\n",
    "    #######################################\n",
    "\n",
    "    return h, cache\n",
    "\n",
    "\n",
    "def lstm_backward(dh, cache):\n",
    "    \"\"\"\n",
    "  LSTM反向传播\n",
    "  Inputs:\n",
    "  - dh: 各隐藏层梯度(N, T, H)\n",
    "  - cache: V前向传播缓存\n",
    "  \n",
    "  Returns 元组:\n",
    "  - dx: 输入数据梯度 (N, T, D)\n",
    "  - dh0:初始隐藏层梯度(N, H)\n",
    "  - dWx: 输入层到隐藏层权重梯度 (D, 4H)\n",
    "  - dWh: 隐藏层到隐藏层权重梯度 (H, 4H)\n",
    "  - db: 偏置项梯度 (4H,)\n",
    "  \"\"\"\n",
    "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "    #######################################\n",
    "    #                 任务：实现完整的LSTM反向传播                              \n",
    "    #######################################\n",
    "    N, T, H = dh.shape\n",
    "    x, Wx, Wh, b, input, input_gate, output_gate, forget_gate, prev_h, prev_c, next_scores_c = cache[\n",
    "        T - 1]\n",
    "    D = x.shape[1]\n",
    "    dprev_h = np.zeros((N, H))\n",
    "    dprev_c = np.zeros((N, H))\n",
    "    dx = np.zeros((N, T, D))\n",
    "    dh0 = np.zeros((N, H))\n",
    "    dWx = np.zeros((D, 4 * H))\n",
    "    dWh = np.zeros((H, 4 * H))\n",
    "    db = np.zeros((4 * H, ))\n",
    "    for t in range(T):\n",
    "        t = T - 1 - t\n",
    "        step_cache = cache[t]\n",
    "        dnext_h = dh[:, t, :] + dprev_h\n",
    "        dnext_c = dprev_c\n",
    "        dx[:, t, :], dprev_h, dprev_c, dWxt, dWht, dbt = lstm_step_backward(\n",
    "            dnext_h, dnext_c, step_cache)\n",
    "        dWx, dWh, db = dWx + dWxt, dWh + dWht, db + dbt\n",
    "    dh0 = dprev_h\n",
    "    #######################################\n",
    "    #                               结束编码                                     #\n",
    "    #######################################\n",
    "\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
