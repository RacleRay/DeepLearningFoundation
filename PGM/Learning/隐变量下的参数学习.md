 隐变量表示的其实是数据的不完整性，也就是训练数据并不能给出关于模型结果的全部信息，因此只能对模型中未知的状态做出概率性的推测。 



## **期望最大化算法**（expectation-maximization algorithm, EM） 

**期望最大化算法**（expectation-maximization algorithm,  EM）是用于计算最大似然估计的迭代方法

- 期望步骤（expectation  step）利用当前的参数来生成关于**隐变量概率的期望函数**
- 最大化步骤（maximization  step）则寻找**让期望函数最大的一组参数**，并将这组参数应用到下一轮的期望步骤中。 



 **EM 算法虽然可以在不能直接求解方程时找到统计模型的最大似然参数，但它并不能保证收敛到全局最优**。



 **在数学上，EM 算法通过不断地局部逼近来解决似然概率最大化的问题**。 



### 高斯混合模型求解简述

假定模型中未知的参数为 $\theta$，隐藏的状态变量为 $Z$，输出的因变量为 $Y$，那么三者就构成了一个马尔可夫链 $\theta \rightarrow Z \rightarrow Y$。EM 算法相当于是通过 $p(z|\theta)$ 的最大化来简化  $p(y|\theta)$ 的最大化。

高斯混合模型（Gaussian mixture model）是由 $K$ 个高斯分布混合而成的模型。

> 作为一个生成模型，高斯混合先按照概率 $\pi_k$ 选择第 $k$  个高斯分布，再按照这个分布的概率密度采出一个样本，因此**高斯分布的选择**和**样本的取值**共同构成了混合模型的完备数据（complete  data）。
>
> 但从观察者的角度看，分布的选择是在生成数据的黑箱里完成的，所以需要用**隐变量 $\bf z$** 来定义，**单独的观测数据 $\bf x$**  就只能构成**不完备数据（**incomplete data）。



#### 求解目标

对高斯混合模型的学习就是在给定不完备数据 $\bf X$ 时，估计模型中所有的 $\pi_k$、$\mu_k$ 和  $\sigma_k$，这些未知的参数可以统称为 $\boldsymbol \theta$。最优的参数 $\boldsymbol \theta$  应该让对数似然函数 $\log p({\bf X} | \boldsymbol \theta)$ 最大化，其数学表达式可以写成

$ L(\boldsymbol \theta | {\bf X}) = \log p({\bf X} | \boldsymbol \theta) = \log \prod\limits_{n=1}^N p({\bf x}_n | \boldsymbol \theta) = \sum\limits_{n=1}^N \log (\sum\limits_{k=1}^K  \pi_k \mathscr{N}({\bf x}_n | \boldsymbol \mu_k, \boldsymbol \Sigma_k)) $

简化：

引入隐变量能够确定唯一的分布，也就是去掉上面表达式中对成分 $k$ 的求和，从而避免对求和项的复杂对数运算。

如果已知每个样本  ${\bf x}_n$ 所对应的隐变量 $z_{nk} = 1$，那就意味着第 $n$ 个样本由第 $k$  个混合成分产生，上面的表达式就可以简化为

$ L(\boldsymbol \theta | {\bf X}, {\bf  Z}) = \sum\limits_{n=1}^N \log \pi_k \mathscr{N}({\bf x}_n | \boldsymbol \mu_k, \boldsymbol \Sigma_k) $



#### 求解步骤

step 1：

隐变量本身也是随机变量，只能用概率描述。如果将参数当前的估计值 $\boldsymbol \theta^{(t)}$  看作真实值，它就可以和不完备数据结合起来，用于估计隐变量的分布。

隐变量的分布可以利用贝叶斯定理计算，将**混合参数 $\pi_k$  看作先验概率**，单个的**高斯分布 $\mathscr{N}(\boldsymbol \mu_k, \boldsymbol \Sigma_k)$  看作似然概率**，就不难计算出隐变量 $z_{nk}$ 关于 $k$ 的后验概率

$ p(z_{nk} | {\bf x}_n, \boldsymbol  \theta^{(t)}) = \dfrac{\pi_k \mathscr{N}({\bf x_n} | \boldsymbol  \theta^{(t)})}{\sum\limits_{j = 1}^K \pi_j \mathscr{N} ({\bf x_n} |  \boldsymbol \theta^{(t)})} $

> 这个后验概率就是其中提到的 " 责任 $\gamma_{nk}$"，其意义是第 $k$  个高斯分布对样本的响应度（responsibility）。
>
> 由于这里计算出的后验是随机变量 $z_{nk} = 1$ 的概率，它实际上代表的就是  $z_{nk}$ 的数学期望。



step 2：

有了隐变量的后验概率，就可以将它代入到基于完备信息的对数似然概率中，通过求和对隐变量进行边际化的处理。

求出的目标**对数似然  $L(\boldsymbol \theta | {\bf X}, {\bf Z})$ 关于隐变量 $\bf Z$ 的数学期望也叫作 $Q$  函数**，其数学表达式为

$ Q(\boldsymbol \theta, \boldsymbol  \theta^{(t)}) = \sum\limits_{\bf Z} p({\bf Z} | {\bf X}, \boldsymbol  \theta^{(t)}) L(\boldsymbol \theta | {\bf X}, {\bf Z}) $

其中 $p({\bf Z} | {\bf X}, \boldsymbol \theta^{(t)}) = \prod_{n=1}^N p(z_{nk} | {\bf x}_n, \boldsymbol \theta^{(t)})$。



step 3：

完备数据下对数似然的数学期望（$p(z_{nk} | {\bf x}_n, \boldsymbol  \theta^{(t)})$ 和 $L(\boldsymbol \theta | {\bf X}, {\bf  Z})$ 带入 $Q(\boldsymbol \theta, \boldsymbol  \theta^{(t)})$）

$ Q(\boldsymbol \theta, \boldsymbol  \theta^{(t)}) = \sum\limits_{n=1}^N \sum\limits_{k=1}^K \gamma_{nk}  [\log \pi_k + \log \mathscr{N}({\bf x}_n | \boldsymbol \mu_k,  \boldsymbol \Sigma_k)] $

这是期望步骤的最终结果。

最大化步骤需要找到让上面的表达式最大化的新参数 $\boldsymbol  \theta^{(t+1)}$，对 $\pi_k$、$\boldsymbol \mu_k$ 和 $\boldsymbol  \Sigma_k$ 分别求偏导数就可以了。



## 应用

 在 Scikit-learn 中，EM 算法被内嵌在 mixture 模块中的 GaussianMixture 类中，调用这个类就调用了 EM 算法。 

```python
import pandas as pd
import numpy as np
from scipy import linalg
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn import mixture

rawstat = pd.read_table('clustering.csv')
stat = pd.DataFrame(rawstat.iloc[:,0])
stat['pass_ratio'] = rawstat.iloc[:,1] / rawstat.iloc[:,2]
stat['dribble_ratio'] = rawstat.iloc[:,3] / rawstat.iloc[:,4]

mixture_stat = stat.iloc[:,[1,2]].values
gmm_model = mixture.GaussianMixture(n_components=3).fit(mixture_stat)
gmm_result = gmm_model.predict(mixture_stat)

fig = plt.figure()
fig = fig.gca()
for i, (mean, covar, color) in enumerate(zip(
            gmm_model.means_, gmm_model.covariances_, (['r','g','b']))):

        plt.scatter(mixture_stat[gmm_result == i, 0], mixture_stat[gmm_result == i, 1], 2, color=color)

        # Plot an ellipse for every Gaussian component
        v, w = linalg.eigh(covar)
        v = 2. * np.sqrt(2.) * np.sqrt(v)
        u = w[0] / linalg.norm(w[0])
        angle = np.arctan(u[1] / u[0])
        angle = 180. * angle / np.pi
        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
        ell.set_clip_box(fig.bbox)
        ell.set_alpha(0.5)
        fig.add_artist(ell)
        for a,b,c in zip(stat['Team'],stat['pass_ratio'], stat['dribble_ratio']):  
            fig.text(b, c+0.005, a, ha='center', va= 'bottom',fontsize=9) 
plt.title("Gaussian Mixture Result")
plt.show()

```



## 总结

- 期望最大化算法通过迭代来求解令观测结果似然概率最大化的未知参数；
- 期望步骤计算完备数据的似然概率关于隐变量的数学期望；
- 最大化步骤通过最大化期望步骤的结果来计算新的参数估计值；
- 期望最大化算法主要用于高斯混合模型等含有隐变量的概率图模型的学习。

 除了高斯混合模型之外，对隐马尔可夫网络的学习也需要使用 EM 算法。在隐马尔可夫的文献中，EM 算法通常被称为 Baum-Welch 算法（Baum-Welch algorithm）。两者虽然名称不同，但原理是一样的。 

